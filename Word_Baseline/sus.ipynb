{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3768034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xia\\AppData\\Local\\Temp\\ipykernel_4220\\1157685092.py:8: DeprecationWarning: Please use `loadmat` from the `scipy.io.matlab` namespace, the `scipy.io.matlab.mio` namespace is deprecated.\n",
      "  from scipy.io.matlab.mio import loadmat, savemat\n",
      "C:\\Users\\xia\\AppData\\Local\\Temp\\ipykernel_4220\\1157685092.py:8: DeprecationWarning: Please use `savemat` from the `scipy.io.matlab` namespace, the `scipy.io.matlab.mio` namespace is deprecated.\n",
      "  from scipy.io.matlab.mio import loadmat, savemat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.io.matlab.mio import loadmat, savemat\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce65bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filename, remove_duplicata = True):\n",
    "    with open(filename) as f:\n",
    "        df = pd.read_csv(f)\n",
    "        if remove_duplicata:\n",
    "            df = df.drop_duplicates()\n",
    "        X = df.to_numpy()\n",
    "    return X[:,:-1], X[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "540b1383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['selectAll', 'selectL', 'selectR', 'selectWR'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = getData(\"dataSelection.csv\")\n",
    "np.unique(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9527872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "tmp = X[:,X.shape[1]//2:]\n",
    "anotherX = X[:,:X.shape[1]//2]-tmp\n",
    "anotherX = anotherX.astype('int64')\n",
    "#anotherX = normalize(anotherX)\n",
    "le.fit(Y)\n",
    "Ynum = le.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62b80b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-783.,    0.,   -1.,   -4.,   -9.]), tensor(0))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = torch.utils.data.TensorDataset(torch.Tensor(anotherX),torch.Tensor(Ynum).type(torch.LongTensor))\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9a57c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2046c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_tanH_stack = nn.Sequential(\n",
    "            nn.Linear(5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, len(np.unique(Y)))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_tanH_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ee2d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e08e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84729ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidation : 0\n",
      "-------------------------------\n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.355906  [    1/ 3296]\n",
      "loss: 0.838001  [  101/ 3296]\n",
      "loss: 1.324145  [  201/ 3296]\n",
      "loss: 1.587722  [  301/ 3296]\n",
      "loss: 1.320586  [  401/ 3296]\n",
      "loss: 1.306448  [  501/ 3296]\n",
      "loss: 1.573415  [  601/ 3296]\n",
      "loss: 1.516358  [  701/ 3296]\n",
      "loss: 1.036168  [  801/ 3296]\n",
      "loss: 1.527487  [  901/ 3296]\n",
      "loss: 1.503439  [ 1001/ 3296]\n",
      "loss: 0.000000  [ 1101/ 3296]\n",
      "loss: 1.563835  [ 1201/ 3296]\n",
      "loss: 1.454279  [ 1301/ 3296]\n",
      "loss: 0.803533  [ 1401/ 3296]\n",
      "loss: 1.605122  [ 1501/ 3296]\n",
      "loss: 1.158620  [ 1601/ 3296]\n",
      "loss: 0.000005  [ 1701/ 3296]\n",
      "loss: 1.527503  [ 1801/ 3296]\n",
      "loss: 0.975378  [ 1901/ 3296]\n",
      "loss: 0.000000  [ 2001/ 3296]\n",
      "loss: 0.097202  [ 2101/ 3296]\n",
      "loss: 1.127797  [ 2201/ 3296]\n",
      "loss: 0.759515  [ 2301/ 3296]\n",
      "loss: 1.422381  [ 2401/ 3296]\n",
      "loss: 1.782738  [ 2501/ 3296]\n",
      "loss: 0.626416  [ 2601/ 3296]\n",
      "loss: 1.438238  [ 2701/ 3296]\n",
      "loss: 1.439865  [ 2801/ 3296]\n",
      "loss: 1.401365  [ 2901/ 3296]\n",
      "loss: 1.088421  [ 3001/ 3296]\n",
      "loss: 0.822146  [ 3101/ 3296]\n",
      "loss: 1.069258  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.966132 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.369555  [    1/ 3296]\n",
      "loss: 0.000000  [  101/ 3296]\n",
      "loss: 1.037917  [  201/ 3296]\n",
      "loss: 0.717583  [  301/ 3296]\n",
      "loss: 1.378018  [  401/ 3296]\n",
      "loss: 0.000000  [  501/ 3296]\n",
      "loss: 0.000000  [  601/ 3296]\n",
      "loss: 2.004017  [  701/ 3296]\n",
      "loss: 0.777045  [  801/ 3296]\n",
      "loss: 1.399141  [  901/ 3296]\n",
      "loss: 2.541543  [ 1001/ 3296]\n",
      "loss: 1.364332  [ 1101/ 3296]\n",
      "loss: 1.966310  [ 1201/ 3296]\n",
      "loss: 0.000000  [ 1301/ 3296]\n",
      "loss: 1.397401  [ 1401/ 3296]\n",
      "loss: 0.000000  [ 1501/ 3296]\n",
      "loss: 1.003642  [ 1601/ 3296]\n",
      "loss: 0.064371  [ 1701/ 3296]\n",
      "loss: 0.609714  [ 1801/ 3296]\n",
      "loss: 0.000204  [ 1901/ 3296]\n",
      "loss: 0.953307  [ 2001/ 3296]\n",
      "loss: 0.000000  [ 2101/ 3296]\n",
      "loss: 1.361913  [ 2201/ 3296]\n",
      "loss: 0.953550  [ 2301/ 3296]\n",
      "loss: 1.361745  [ 2401/ 3296]\n",
      "loss: 0.943732  [ 2501/ 3296]\n",
      "loss: 0.000000  [ 2601/ 3296]\n",
      "loss: 0.966666  [ 2701/ 3296]\n",
      "loss: 0.000000  [ 2801/ 3296]\n",
      "loss: 0.584015  [ 2901/ 3296]\n",
      "loss: 0.876007  [ 3001/ 3296]\n",
      "loss: 1.318326  [ 3101/ 3296]\n",
      "loss: 1.333746  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.910774 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.906626  [    1/ 3296]\n",
      "loss: 1.327281  [  101/ 3296]\n",
      "loss: 1.488302  [  201/ 3296]\n",
      "loss: 1.383316  [  301/ 3296]\n",
      "loss: 1.182956  [  401/ 3296]\n",
      "loss: 0.852729  [  501/ 3296]\n",
      "loss: 0.662617  [  601/ 3296]\n",
      "loss: 0.957589  [  701/ 3296]\n",
      "loss: 0.889723  [  801/ 3296]\n",
      "loss: 0.000000  [  901/ 3296]\n",
      "loss: 1.401276  [ 1001/ 3296]\n",
      "loss: 1.282567  [ 1101/ 3296]\n",
      "loss: 0.000000  [ 1201/ 3296]\n",
      "loss: 0.946933  [ 1301/ 3296]\n",
      "loss: 0.809666  [ 1401/ 3296]\n",
      "loss: 1.255548  [ 1501/ 3296]\n",
      "loss: 0.617081  [ 1601/ 3296]\n",
      "loss: 0.821007  [ 1701/ 3296]\n",
      "loss: 1.134864  [ 1801/ 3296]\n",
      "loss: 0.000000  [ 1901/ 3296]\n",
      "loss: 0.797118  [ 2001/ 3296]\n",
      "loss: 0.832882  [ 2101/ 3296]\n",
      "loss: 1.085396  [ 2201/ 3296]\n",
      "loss: 0.000000  [ 2301/ 3296]\n",
      "loss: 0.860164  [ 2401/ 3296]\n",
      "loss: 0.758428  [ 2501/ 3296]\n",
      "loss: 1.406292  [ 2601/ 3296]\n",
      "loss: 0.885791  [ 2701/ 3296]\n",
      "loss: 0.842966  [ 2801/ 3296]\n",
      "loss: 0.995625  [ 2901/ 3296]\n",
      "loss: 1.366067  [ 3001/ 3296]\n",
      "loss: 1.450264  [ 3101/ 3296]\n",
      "loss: 0.770619  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.844390 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.645100  [    1/ 3296]\n",
      "loss: 0.757922  [  101/ 3296]\n",
      "loss: 0.000000  [  201/ 3296]\n",
      "loss: 0.000000  [  301/ 3296]\n",
      "loss: 0.000000  [  401/ 3296]\n",
      "loss: 0.834175  [  501/ 3296]\n",
      "loss: 0.889473  [  601/ 3296]\n",
      "loss: 0.821270  [  701/ 3296]\n",
      "loss: 1.525204  [  801/ 3296]\n",
      "loss: 0.719139  [  901/ 3296]\n",
      "loss: 0.000971  [ 1001/ 3296]\n",
      "loss: 1.306616  [ 1101/ 3296]\n",
      "loss: 0.963060  [ 1201/ 3296]\n",
      "loss: 0.521923  [ 1301/ 3296]\n",
      "loss: 0.758189  [ 1401/ 3296]\n",
      "loss: 0.000000  [ 1501/ 3296]\n",
      "loss: 1.368968  [ 1601/ 3296]\n",
      "loss: 0.000065  [ 1701/ 3296]\n",
      "loss: 0.000000  [ 1801/ 3296]\n",
      "loss: 0.762687  [ 1901/ 3296]\n",
      "loss: 0.530513  [ 2001/ 3296]\n",
      "loss: 1.529451  [ 2101/ 3296]\n",
      "loss: 0.000000  [ 2201/ 3296]\n",
      "loss: 0.639305  [ 2301/ 3296]\n",
      "loss: 1.476685  [ 2401/ 3296]\n",
      "loss: 0.745031  [ 2501/ 3296]\n",
      "loss: 0.000000  [ 2601/ 3296]\n",
      "loss: 0.595485  [ 2701/ 3296]\n",
      "loss: 1.377537  [ 2801/ 3296]\n",
      "loss: 0.754928  [ 2901/ 3296]\n",
      "loss: 0.000000  [ 3001/ 3296]\n",
      "loss: 0.071700  [ 3101/ 3296]\n",
      "loss: 0.000000  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.779340 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.894084  [    1/ 3296]\n",
      "loss: 1.302384  [  101/ 3296]\n",
      "loss: 0.000000  [  201/ 3296]\n",
      "loss: 0.000000  [  301/ 3296]\n",
      "loss: 0.705828  [  401/ 3296]\n",
      "loss: 0.684136  [  501/ 3296]\n",
      "loss: 0.780341  [  601/ 3296]\n",
      "loss: 0.720536  [  701/ 3296]\n",
      "loss: 1.650707  [  801/ 3296]\n",
      "loss: 0.689354  [  901/ 3296]\n",
      "loss: 0.724860  [ 1001/ 3296]\n",
      "loss: 0.567409  [ 1101/ 3296]\n",
      "loss: 0.000000  [ 1201/ 3296]\n",
      "loss: 0.817729  [ 1301/ 3296]\n",
      "loss: 0.000000  [ 1401/ 3296]\n",
      "loss: 1.307183  [ 1501/ 3296]\n",
      "loss: 1.661117  [ 1601/ 3296]\n",
      "loss: 0.222023  [ 1701/ 3296]\n",
      "loss: 0.421948  [ 1801/ 3296]\n",
      "loss: 0.234657  [ 1901/ 3296]\n",
      "loss: 0.000000  [ 2001/ 3296]\n",
      "loss: 0.000013  [ 2101/ 3296]\n",
      "loss: 0.719783  [ 2201/ 3296]\n",
      "loss: 0.488348  [ 2301/ 3296]\n",
      "loss: 0.057728  [ 2401/ 3296]\n",
      "loss: 1.350017  [ 2501/ 3296]\n",
      "loss: 0.000000  [ 2601/ 3296]\n",
      "loss: 0.000000  [ 2701/ 3296]\n",
      "loss: 0.000000  [ 2801/ 3296]\n",
      "loss: 1.569162  [ 2901/ 3296]\n",
      "loss: 0.992050  [ 3001/ 3296]\n",
      "loss: 0.686266  [ 3101/ 3296]\n",
      "loss: 0.666936  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.732390 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.550574  [    1/ 3296]\n",
      "loss: 0.299585  [  101/ 3296]\n",
      "loss: 0.538237  [  201/ 3296]\n",
      "loss: 1.097214  [  301/ 3296]\n",
      "loss: 1.268426  [  401/ 3296]\n",
      "loss: 1.240806  [  501/ 3296]\n",
      "loss: 1.273487  [  601/ 3296]\n",
      "loss: 1.200880  [  701/ 3296]\n",
      "loss: 1.272763  [  801/ 3296]\n",
      "loss: 0.203104  [  901/ 3296]\n",
      "loss: 0.683504  [ 1001/ 3296]\n",
      "loss: 0.687829  [ 1101/ 3296]\n",
      "loss: 0.000000  [ 1201/ 3296]\n",
      "loss: 0.000000  [ 1301/ 3296]\n",
      "loss: 0.627046  [ 1401/ 3296]\n",
      "loss: 0.651953  [ 1501/ 3296]\n",
      "loss: 0.623343  [ 1601/ 3296]\n",
      "loss: 0.000000  [ 1701/ 3296]\n",
      "loss: 1.819678  [ 1801/ 3296]\n",
      "loss: 1.327684  [ 1901/ 3296]\n",
      "loss: 1.247520  [ 2001/ 3296]\n",
      "loss: 0.667084  [ 2101/ 3296]\n",
      "loss: 1.655970  [ 2201/ 3296]\n",
      "loss: 0.712944  [ 2301/ 3296]\n",
      "loss: 0.000000  [ 2401/ 3296]\n",
      "loss: 0.000000  [ 2501/ 3296]\n",
      "loss: 1.182856  [ 2601/ 3296]\n",
      "loss: 1.260959  [ 2701/ 3296]\n",
      "loss: 0.220484  [ 2801/ 3296]\n",
      "loss: 0.004014  [ 2901/ 3296]\n",
      "loss: 1.082466  [ 3001/ 3296]\n",
      "loss: 0.461875  [ 3101/ 3296]\n",
      "loss: 0.289677  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.704085 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.233834  [    1/ 3296]\n",
      "loss: 0.720354  [  101/ 3296]\n",
      "loss: 0.650309  [  201/ 3296]\n",
      "loss: 1.807973  [  301/ 3296]\n",
      "loss: 1.261701  [  401/ 3296]\n",
      "loss: 0.120923  [  501/ 3296]\n",
      "loss: 0.311967  [  601/ 3296]\n",
      "loss: 0.225498  [  701/ 3296]\n",
      "loss: 1.151913  [  801/ 3296]\n",
      "loss: 0.106199  [  901/ 3296]\n",
      "loss: 1.817496  [ 1001/ 3296]\n",
      "loss: 0.000000  [ 1101/ 3296]\n",
      "loss: 0.700267  [ 1201/ 3296]\n",
      "loss: 1.376082  [ 1301/ 3296]\n",
      "loss: 0.000000  [ 1401/ 3296]\n",
      "loss: 0.635308  [ 1501/ 3296]\n",
      "loss: 1.139226  [ 1601/ 3296]\n",
      "loss: 0.000000  [ 1701/ 3296]\n",
      "loss: 0.000000  [ 1801/ 3296]\n",
      "loss: 0.059045  [ 1901/ 3296]\n",
      "loss: 0.262191  [ 2001/ 3296]\n",
      "loss: 1.151767  [ 2101/ 3296]\n",
      "loss: 0.703475  [ 2201/ 3296]\n",
      "loss: 0.026005  [ 2301/ 3296]\n",
      "loss: 1.708760  [ 2401/ 3296]\n",
      "loss: 1.105779  [ 2501/ 3296]\n",
      "loss: 1.402470  [ 2601/ 3296]\n",
      "loss: 1.114178  [ 2701/ 3296]\n",
      "loss: 1.844389  [ 2801/ 3296]\n",
      "loss: 1.261314  [ 2901/ 3296]\n",
      "loss: 1.099678  [ 3001/ 3296]\n",
      "loss: 0.662366  [ 3101/ 3296]\n",
      "loss: 1.336304  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.662793 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.723532  [    1/ 3296]\n",
      "loss: 1.960581  [  101/ 3296]\n",
      "loss: 0.000000  [  201/ 3296]\n",
      "loss: 0.000000  [  301/ 3296]\n",
      "loss: 0.649087  [  401/ 3296]\n",
      "loss: 0.000000  [  501/ 3296]\n",
      "loss: 1.068967  [  601/ 3296]\n",
      "loss: 0.265291  [  701/ 3296]\n",
      "loss: 0.698907  [  801/ 3296]\n",
      "loss: 1.050633  [  901/ 3296]\n",
      "loss: 0.000000  [ 1001/ 3296]\n",
      "loss: 0.693870  [ 1101/ 3296]\n",
      "loss: 0.127642  [ 1201/ 3296]\n",
      "loss: 0.741886  [ 1301/ 3296]\n",
      "loss: 0.000000  [ 1401/ 3296]\n",
      "loss: 1.066367  [ 1501/ 3296]\n",
      "loss: 0.619261  [ 1601/ 3296]\n",
      "loss: 3.046310  [ 1701/ 3296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.054452  [ 1801/ 3296]\n",
      "loss: 0.278739  [ 1901/ 3296]\n",
      "loss: 0.176567  [ 2001/ 3296]\n",
      "loss: 1.417716  [ 2101/ 3296]\n",
      "loss: 1.036903  [ 2201/ 3296]\n",
      "loss: 0.697073  [ 2301/ 3296]\n",
      "loss: 2.059333  [ 2401/ 3296]\n",
      "loss: 1.040936  [ 2501/ 3296]\n",
      "loss: 0.116423  [ 2601/ 3296]\n",
      "loss: 0.636410  [ 2701/ 3296]\n",
      "loss: 2.064877  [ 2801/ 3296]\n",
      "loss: 0.000000  [ 2901/ 3296]\n",
      "loss: 0.566903  [ 3001/ 3296]\n",
      "loss: 0.187568  [ 3101/ 3296]\n",
      "loss: 0.258840  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.651522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.262958  [    1/ 3296]\n",
      "loss: 0.167101  [  101/ 3296]\n",
      "loss: 1.542018  [  201/ 3296]\n",
      "loss: 1.679717  [  301/ 3296]\n",
      "loss: 1.748566  [  401/ 3296]\n",
      "loss: 1.385269  [  501/ 3296]\n",
      "loss: 0.600202  [  601/ 3296]\n",
      "loss: 0.958960  [  701/ 3296]\n",
      "loss: 0.772909  [  801/ 3296]\n",
      "loss: 0.790300  [  901/ 3296]\n",
      "loss: 0.000000  [ 1001/ 3296]\n",
      "loss: 0.943702  [ 1101/ 3296]\n",
      "loss: 0.604659  [ 1201/ 3296]\n",
      "loss: 1.086888  [ 1301/ 3296]\n",
      "loss: 0.402440  [ 1401/ 3296]\n",
      "loss: 0.643422  [ 1501/ 3296]\n",
      "loss: 1.799237  [ 1601/ 3296]\n",
      "loss: 0.245943  [ 1701/ 3296]\n",
      "loss: 0.319592  [ 1801/ 3296]\n",
      "loss: 1.230510  [ 1901/ 3296]\n",
      "loss: 0.000000  [ 2001/ 3296]\n",
      "loss: 0.000000  [ 2101/ 3296]\n",
      "loss: 0.680489  [ 2201/ 3296]\n",
      "loss: 0.274142  [ 2301/ 3296]\n",
      "loss: 0.807355  [ 2401/ 3296]\n",
      "loss: 0.676650  [ 2501/ 3296]\n",
      "loss: 0.787598  [ 2601/ 3296]\n",
      "loss: 0.757585  [ 2701/ 3296]\n",
      "loss: 0.625900  [ 2801/ 3296]\n",
      "loss: 0.538942  [ 2901/ 3296]\n",
      "loss: 0.604523  [ 3001/ 3296]\n",
      "loss: 0.024272  [ 3101/ 3296]\n",
      "loss: 0.000000  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.648128 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.004751  [    1/ 3296]\n",
      "loss: 2.149692  [  101/ 3296]\n",
      "loss: 0.000000  [  201/ 3296]\n",
      "loss: 0.964797  [  301/ 3296]\n",
      "loss: 0.060020  [  401/ 3296]\n",
      "loss: 0.660187  [  501/ 3296]\n",
      "loss: 0.000000  [  601/ 3296]\n",
      "loss: 0.131934  [  701/ 3296]\n",
      "loss: 1.644340  [  801/ 3296]\n",
      "loss: 0.661874  [  901/ 3296]\n",
      "loss: 0.000000  [ 1001/ 3296]\n",
      "loss: 0.169751  [ 1101/ 3296]\n",
      "loss: 0.119548  [ 1201/ 3296]\n",
      "loss: 0.244201  [ 1301/ 3296]\n",
      "loss: 0.764381  [ 1401/ 3296]\n",
      "loss: 0.001672  [ 1501/ 3296]\n",
      "loss: 0.548057  [ 1601/ 3296]\n",
      "loss: 0.995937  [ 1701/ 3296]\n",
      "loss: 2.147543  [ 1801/ 3296]\n",
      "loss: 1.146510  [ 1901/ 3296]\n",
      "loss: 0.672709  [ 2001/ 3296]\n",
      "loss: 0.761940  [ 2101/ 3296]\n",
      "loss: 0.630979  [ 2201/ 3296]\n",
      "loss: 0.913582  [ 2301/ 3296]\n",
      "loss: 0.750729  [ 2401/ 3296]\n",
      "loss: 0.219068  [ 2501/ 3296]\n",
      "loss: 0.579575  [ 2601/ 3296]\n",
      "loss: 0.941328  [ 2701/ 3296]\n",
      "loss: 0.047750  [ 2801/ 3296]\n",
      "loss: 1.526039  [ 2901/ 3296]\n",
      "loss: 0.000000  [ 3001/ 3296]\n",
      "loss: 0.743821  [ 3101/ 3296]\n",
      "loss: 1.868646  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.653573 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.000000  [    1/ 3296]\n",
      "loss: 0.000000  [  101/ 3296]\n",
      "loss: 0.039585  [  201/ 3296]\n",
      "loss: 0.020753  [  301/ 3296]\n",
      "loss: 1.666018  [  401/ 3296]\n",
      "loss: 1.777003  [  501/ 3296]\n",
      "loss: 0.119283  [  601/ 3296]\n",
      "loss: 0.813606  [  701/ 3296]\n",
      "loss: 0.797997  [  801/ 3296]\n",
      "loss: 0.000000  [  901/ 3296]\n",
      "loss: 0.061287  [ 1001/ 3296]\n",
      "loss: 0.813165  [ 1101/ 3296]\n",
      "loss: 0.042011  [ 1201/ 3296]\n",
      "loss: 0.648375  [ 1301/ 3296]\n",
      "loss: 0.827199  [ 1401/ 3296]\n",
      "loss: 2.828170  [ 1501/ 3296]\n",
      "loss: 0.000009  [ 1601/ 3296]\n",
      "loss: 0.178741  [ 1701/ 3296]\n",
      "loss: 1.274147  [ 1801/ 3296]\n",
      "loss: 0.941721  [ 1901/ 3296]\n",
      "loss: 0.884725  [ 2001/ 3296]\n",
      "loss: 0.782151  [ 2101/ 3296]\n",
      "loss: 0.597098  [ 2201/ 3296]\n",
      "loss: 2.116207  [ 2301/ 3296]\n",
      "loss: 0.000000  [ 2401/ 3296]\n",
      "loss: 0.606140  [ 2501/ 3296]\n",
      "loss: 0.092731  [ 2601/ 3296]\n",
      "loss: 1.602958  [ 2701/ 3296]\n",
      "loss: 0.000000  [ 2801/ 3296]\n",
      "loss: 1.278481  [ 2901/ 3296]\n",
      "loss: 0.193933  [ 3001/ 3296]\n",
      "loss: 0.742229  [ 3101/ 3296]\n",
      "loss: 2.302612  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.664573 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.612911  [    1/ 3296]\n",
      "loss: 0.811266  [  101/ 3296]\n",
      "loss: 0.008433  [  201/ 3296]\n",
      "loss: 2.528661  [  301/ 3296]\n",
      "loss: 0.000000  [  401/ 3296]\n",
      "loss: 0.593810  [  501/ 3296]\n",
      "loss: 0.219313  [  601/ 3296]\n",
      "loss: 0.000000  [  701/ 3296]\n",
      "loss: 4.091928  [  801/ 3296]\n",
      "loss: 1.392650  [  901/ 3296]\n",
      "loss: 0.000000  [ 1001/ 3296]\n",
      "loss: 4.100678  [ 1101/ 3296]\n",
      "loss: 0.326131  [ 1201/ 3296]\n",
      "loss: 0.673383  [ 1301/ 3296]\n",
      "loss: 2.335316  [ 1401/ 3296]\n",
      "loss: 1.783882  [ 1501/ 3296]\n",
      "loss: 0.014377  [ 1601/ 3296]\n",
      "loss: 0.000000  [ 1701/ 3296]\n",
      "loss: 0.798208  [ 1801/ 3296]\n",
      "loss: 1.709967  [ 1901/ 3296]\n",
      "loss: 0.697072  [ 2001/ 3296]\n",
      "loss: 0.112275  [ 2101/ 3296]\n",
      "loss: 0.761395  [ 2201/ 3296]\n",
      "loss: 0.637305  [ 2301/ 3296]\n",
      "loss: 0.069254  [ 2401/ 3296]\n",
      "loss: 0.613541  [ 2501/ 3296]\n",
      "loss: 0.350149  [ 2601/ 3296]\n",
      "loss: 0.762963  [ 2701/ 3296]\n",
      "loss: 0.000000  [ 2801/ 3296]\n",
      "loss: 0.000000  [ 2901/ 3296]\n",
      "loss: 0.756854  [ 3001/ 3296]\n",
      "loss: 0.199165  [ 3101/ 3296]\n",
      "loss: 0.000000  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.274854 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.009660  [    1/ 3296]\n",
      "loss: 0.000000  [  101/ 3296]\n",
      "loss: 0.000000  [  201/ 3296]\n",
      "loss: 0.000000  [  301/ 3296]\n",
      "loss: 0.000000  [  401/ 3296]\n",
      "loss: 0.814050  [  501/ 3296]\n",
      "loss: 0.024347  [  601/ 3296]\n",
      "loss: 0.745280  [  701/ 3296]\n",
      "loss: 0.013117  [  801/ 3296]\n",
      "loss: 1.744459  [  901/ 3296]\n",
      "loss: 0.877625  [ 1001/ 3296]\n",
      "loss: 0.014701  [ 1101/ 3296]\n",
      "loss: 0.598687  [ 1201/ 3296]\n",
      "loss: 0.000004  [ 1301/ 3296]\n",
      "loss: 0.712455  [ 1401/ 3296]\n",
      "loss: 0.061570  [ 1501/ 3296]\n",
      "loss: 0.631580  [ 1601/ 3296]\n",
      "loss: 0.000000  [ 1701/ 3296]\n",
      "loss: 0.664404  [ 1801/ 3296]\n",
      "loss: 0.719644  [ 1901/ 3296]\n",
      "loss: 0.027440  [ 2001/ 3296]\n",
      "loss: 0.757457  [ 2101/ 3296]\n",
      "loss: 0.703628  [ 2201/ 3296]\n",
      "loss: 1.505047  [ 2301/ 3296]\n",
      "loss: 1.465015  [ 2401/ 3296]\n",
      "loss: 0.663122  [ 2501/ 3296]\n",
      "loss: 1.614685  [ 2601/ 3296]\n",
      "loss: 0.769384  [ 2701/ 3296]\n",
      "loss: 0.793850  [ 2801/ 3296]\n",
      "loss: 0.874660  [ 2901/ 3296]\n",
      "loss: 0.695690  [ 3001/ 3296]\n",
      "loss: 0.895801  [ 3101/ 3296]\n",
      "loss: 0.696133  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.668533 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.107627  [    1/ 3296]\n",
      "loss: 0.001266  [  101/ 3296]\n",
      "loss: 0.493283  [  201/ 3296]\n",
      "loss: 0.708587  [  301/ 3296]\n",
      "loss: 0.099907  [  401/ 3296]\n",
      "loss: 0.171025  [  501/ 3296]\n",
      "loss: 0.000000  [  601/ 3296]\n",
      "loss: 0.650550  [  701/ 3296]\n",
      "loss: 0.153295  [  801/ 3296]\n",
      "loss: 0.000000  [  901/ 3296]\n",
      "loss: 0.553511  [ 1001/ 3296]\n",
      "loss: 1.862553  [ 1101/ 3296]\n",
      "loss: 0.000000  [ 1201/ 3296]\n",
      "loss: 0.036183  [ 1301/ 3296]\n",
      "loss: 0.672246  [ 1401/ 3296]\n",
      "loss: 0.000000  [ 1501/ 3296]\n",
      "loss: 0.223767  [ 1601/ 3296]\n",
      "loss: 0.007004  [ 1701/ 3296]\n",
      "loss: 0.000000  [ 1801/ 3296]\n",
      "loss: 1.270082  [ 1901/ 3296]\n",
      "loss: 1.018767  [ 2001/ 3296]\n",
      "loss: 2.337784  [ 2101/ 3296]\n",
      "loss: 0.036062  [ 2201/ 3296]\n",
      "loss: 0.849651  [ 2301/ 3296]\n",
      "loss: 1.001629  [ 2401/ 3296]\n",
      "loss: 0.169392  [ 2501/ 3296]\n",
      "loss: 0.727985  [ 2601/ 3296]\n",
      "loss: 0.000000  [ 2701/ 3296]\n",
      "loss: 0.723289  [ 2801/ 3296]\n",
      "loss: 0.662390  [ 2901/ 3296]\n",
      "loss: 0.159901  [ 3001/ 3296]\n",
      "loss: 0.461994  [ 3101/ 3296]\n",
      "loss: 0.654945  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.676428 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.664701  [    1/ 3296]\n",
      "loss: 2.180063  [  101/ 3296]\n",
      "loss: 0.000000  [  201/ 3296]\n",
      "loss: 2.536534  [  301/ 3296]\n",
      "loss: 0.118662  [  401/ 3296]\n",
      "loss: 0.410920  [  501/ 3296]\n",
      "loss: 0.661303  [  601/ 3296]\n",
      "loss: 2.118612  [  701/ 3296]\n",
      "loss: 1.389376  [  801/ 3296]\n",
      "loss: 0.129961  [  901/ 3296]\n",
      "loss: 0.000000  [ 1001/ 3296]\n",
      "loss: 0.000000  [ 1101/ 3296]\n",
      "loss: 0.179913  [ 1201/ 3296]\n",
      "loss: 0.652245  [ 1301/ 3296]\n",
      "loss: 0.001619  [ 1401/ 3296]\n",
      "loss: 1.081970  [ 1501/ 3296]\n",
      "loss: 0.768306  [ 1601/ 3296]\n",
      "loss: 0.137653  [ 1701/ 3296]\n",
      "loss: 0.919702  [ 1801/ 3296]\n",
      "loss: 0.658869  [ 1901/ 3296]\n",
      "loss: 1.643099  [ 2001/ 3296]\n",
      "loss: 0.413589  [ 2101/ 3296]\n",
      "loss: 0.394307  [ 2201/ 3296]\n",
      "loss: 0.000000  [ 2301/ 3296]\n",
      "loss: 0.744281  [ 2401/ 3296]\n",
      "loss: 1.457674  [ 2501/ 3296]\n",
      "loss: 0.700089  [ 2601/ 3296]\n",
      "loss: 0.014961  [ 2701/ 3296]\n",
      "loss: 0.478461  [ 2801/ 3296]\n",
      "loss: 0.000000  [ 2901/ 3296]\n",
      "loss: 1.503528  [ 3001/ 3296]\n",
      "loss: 0.470665  [ 3101/ 3296]\n",
      "loss: 0.000000  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.701133 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.271843  [    1/ 3296]\n",
      "loss: 0.000241  [  101/ 3296]\n",
      "loss: 1.327894  [  201/ 3296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000000  [  301/ 3296]\n",
      "loss: 0.145567  [  401/ 3296]\n",
      "loss: 0.570648  [  501/ 3296]\n",
      "loss: 0.678440  [  601/ 3296]\n",
      "loss: 0.055141  [  701/ 3296]\n",
      "loss: 0.693052  [  801/ 3296]\n",
      "loss: 0.201043  [  901/ 3296]\n",
      "loss: 0.451609  [ 1001/ 3296]\n",
      "loss: 0.554605  [ 1101/ 3296]\n",
      "loss: 0.705983  [ 1201/ 3296]\n",
      "loss: 0.000000  [ 1301/ 3296]\n",
      "loss: 2.620073  [ 1401/ 3296]\n",
      "loss: 1.757338  [ 1501/ 3296]\n",
      "loss: 0.648105  [ 1601/ 3296]\n",
      "loss: 0.611096  [ 1701/ 3296]\n",
      "loss: 2.589902  [ 1801/ 3296]\n",
      "loss: 1.222552  [ 1901/ 3296]\n",
      "loss: 0.982085  [ 2001/ 3296]\n",
      "loss: 0.000000  [ 2101/ 3296]\n",
      "loss: 0.075291  [ 2201/ 3296]\n",
      "loss: 0.689745  [ 2301/ 3296]\n",
      "loss: 0.552533  [ 2401/ 3296]\n",
      "loss: 0.000000  [ 2501/ 3296]\n",
      "loss: 0.000000  [ 2601/ 3296]\n",
      "loss: 1.328679  [ 2701/ 3296]\n",
      "loss: 0.031548  [ 2801/ 3296]\n",
      "loss: 0.727865  [ 2901/ 3296]\n",
      "loss: 0.003797  [ 3001/ 3296]\n",
      "loss: 2.363805  [ 3101/ 3296]\n",
      "loss: 0.003421  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.699780 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.216327  [    1/ 3296]\n",
      "loss: 0.008780  [  101/ 3296]\n",
      "loss: 0.636177  [  201/ 3296]\n",
      "loss: 2.702152  [  301/ 3296]\n",
      "loss: 0.004168  [  401/ 3296]\n",
      "loss: 0.000000  [  501/ 3296]\n",
      "loss: 0.515328  [  601/ 3296]\n",
      "loss: 0.000000  [  701/ 3296]\n",
      "loss: 0.569117  [  801/ 3296]\n",
      "loss: 1.608040  [  901/ 3296]\n",
      "loss: 0.581370  [ 1001/ 3296]\n",
      "loss: 0.104748  [ 1101/ 3296]\n",
      "loss: 0.628908  [ 1201/ 3296]\n",
      "loss: 0.349938  [ 1301/ 3296]\n",
      "loss: 0.012983  [ 1401/ 3296]\n",
      "loss: 0.567948  [ 1501/ 3296]\n",
      "loss: 1.135109  [ 1601/ 3296]\n",
      "loss: 0.564898  [ 1701/ 3296]\n",
      "loss: 0.606990  [ 1801/ 3296]\n",
      "loss: 0.983664  [ 1901/ 3296]\n",
      "loss: 0.973598  [ 2001/ 3296]\n",
      "loss: 5.726047  [ 2101/ 3296]\n",
      "loss: 0.011198  [ 2201/ 3296]\n",
      "loss: 0.683761  [ 2301/ 3296]\n",
      "loss: 0.018207  [ 2401/ 3296]\n",
      "loss: 0.000000  [ 2501/ 3296]\n",
      "loss: 0.000000  [ 2601/ 3296]\n",
      "loss: 1.365668  [ 2701/ 3296]\n",
      "loss: 0.581926  [ 2801/ 3296]\n",
      "loss: 0.021884  [ 2901/ 3296]\n",
      "loss: 1.900603  [ 3001/ 3296]\n",
      "loss: 0.614033  [ 3101/ 3296]\n",
      "loss: 0.607506  [ 3201/ 3296]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.702299 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.631675  [    1/ 3296]\n",
      "loss: 0.635608  [  101/ 3296]\n",
      "loss: 0.549689  [  201/ 3296]\n",
      "loss: 1.350930  [  301/ 3296]\n",
      "loss: 0.916955  [  401/ 3296]\n",
      "loss: 1.914984  [  501/ 3296]\n",
      "loss: 0.088987  [  601/ 3296]\n",
      "loss: 0.494072  [  701/ 3296]\n",
      "loss: 0.559667  [  801/ 3296]\n",
      "loss: 0.671073  [  901/ 3296]\n",
      "loss: 0.000000  [ 1001/ 3296]\n",
      "loss: 0.632898  [ 1101/ 3296]\n",
      "loss: 0.511750  [ 1201/ 3296]\n",
      "loss: 1.396867  [ 1301/ 3296]\n",
      "loss: 0.000000  [ 1401/ 3296]\n",
      "loss: 0.000000  [ 1501/ 3296]\n",
      "loss: 0.134000  [ 1601/ 3296]\n",
      "loss: 2.011738  [ 1701/ 3296]\n",
      "loss: 1.161220  [ 1801/ 3296]\n",
      "loss: 2.800710  [ 1901/ 3296]\n",
      "loss: 1.028692  [ 2001/ 3296]\n",
      "loss: 0.000000  [ 2101/ 3296]\n",
      "loss: 0.000000  [ 2201/ 3296]\n",
      "loss: 0.000000  [ 2301/ 3296]\n",
      "loss: 0.944374  [ 2401/ 3296]\n",
      "loss: 1.340030  [ 2501/ 3296]\n",
      "loss: 0.025293  [ 2601/ 3296]\n",
      "loss: 0.000000  [ 2701/ 3296]\n",
      "loss: 1.360124  [ 2801/ 3296]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m#test(test_dataloader, model, loss_fn)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     test(test_dataloader, model, loss_fn)\n",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      5\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\STAGE\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\STAGE\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\STAGE\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\STAGE\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\STAGE\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\STAGE\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\STAGE\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\STAGE\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "for i in range(5):\n",
    "    model = NeuralNetwork().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    print(\"CrossValidation : \"+str(i)+\"\\n-------------------------------\\n\")\n",
    "    train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    # Create the data loaders for training and validation\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        #test(test_dataloader, model, loss_fn)\n",
    "        test(test_dataloader, model, loss_fn)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#Used to calculate the confusion matrix\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for X,y in test_dataloader:\n",
    "    y_true.extend(y.data.cpu().numpy())\n",
    "\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    pred = model(X)\n",
    "    y_pred.extend(pred.argmax(1).data.cpu().numpy())\n",
    "    \n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.unique(Y))\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f7dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b40ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
